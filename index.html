<head>
<title>Daisy App Project |ARL Tech. CVIT </title>
<link href="styles.css" rel="stylesheet" type="text/css" >
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
</head>

<body>
<h2>Daisy App : Digital Talking Books Convertor App</h2>
<h3>About Daisy Pipleine</h3>
<p><b>DAISY,</b> which stands for Digital Accessible Information System, is a set of standards developed to make written materials such as books more accessible to people with print disabilities. DAISY provides a way to create digital talking books for those who want to hear and navigate  written material presented in an audible format.
</p>
<h3>Week-wise updation of Project:</h3>

<ul>
	<li><font class="date">Oct 6, 2018</font></li>
	<ul style="margin-bottom: 10px;"> 
<li> <a href="tesser_4_accuracy.html" target="_blank">Accuracy</a> Report of Tesseract v4.0 OCR for English</li> 
<p>Did some changes in Django file, like: </p>
  <li>XML appending will be done after all annotations.</li>
  <li>Annotation button is open for user.</li>
  <li>During uploadation only OCR output is getting saved in server side.</li>
  <li>XML parser button to deploy to detect xml parsing error before TTS input.</li>
  <li>Single uploading can be add to bulk uploaded file (book).</li>
  <li>Can accept any zipped formatted file.</li>		
	</ul>

	<li><font class="date">Sep 25, 2018</font></li>
	<ul style="margin-bottom: 10px;">
		<!---
		<li><a href="text_face_results.html" target="_blank">Text Recognition and Face Detection Results</a></li>
		<li>Have thought of <a href="model_skeleton.html" target="_blank">a rough model skeleton</a> that can extract semantically relevant emotion phrases from existing emotion class descriptions.</li> 
		<li>Feedback by Prof. Jawahar for next week: 
			<ul>
				<li>Things are shaping up.</li>
				<li>Let's figure out generating a sentence description later</li>
				<li>Start looking into data collection; discuss with Prof. PK regarding this.</li>
			</ul></li> -->
	</ul>

	<li><font class="date">Sep 14, 2018</font></li>
	<ul style="margin-bottom: 10px;">
		<!---
		<li> <a href="two_papers_summary.html" target="_blank">Detailed thoughts</a> on two most related papers to our problem. Both of them are efforts to capture more information from the faces than the six basic emotion labels. </li> 
		<li>The emotion description should convey the feelings / emotion of the person in the image without excessively focusing on the specifics of the facial expression.</li>
		<li>Important feedback by Prof. Jawahar for next week: 
			<ul>
				<li>Get basic models ready (text recognition, face detection)</li>
				<li>Stop scratching the surface and go into detail.</li>
				<li>Is there a way to compose new descriptions from existing set of descriptions for a query image?</li>
			</ul></li>
		-->
	</ul>

	<li><font class="date">Sep 6, 2018</font></li>
	<ul style="margin-bottom: 10px;">
		<!---
		<li> The past month has been spent in coming up with research ideas and drafting proposals for each of them. Here's the <a href="files/Meme Project Research proposals (Prajwal).pdf" target="_blank">complete PDF</a> containing 10 proposals for reference. </li>
		<li> The closest proposal for this project: <a href="files/Learning diverse expressions and emotions from Reaction images.pdf" target="_blank"> Learning diverse expressions and emotions from Reaction images </a>. Our final direction extends this idea of moving away from six basic emotions even further. </li> 
		<li> A <a href="conclusions_6_sep.html" target="_blank"> brief summary of the final conclusions </a> after today's all-team meeting.  
	</ul> -->

	
</ul>

</body>
</html> 
